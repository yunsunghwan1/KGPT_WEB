from django.shortcuts import render
from django.http import HttpResponse
from transformers import GPT2LMHeadModel
from transformers import PreTrainedTokenizerFast
import torch
import numpy as np
import pandas as pd
import torch
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
# from pytorch_lightning.core.lightning import LightningModule
from torch.utils.data import DataLoader, Dataset
from transformers.optimization import AdamW, get_cosine_schedule_with_warmup
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
import re
import urllib.request

def index(request) :
    return render(request,
                  "GPTapp/index.html",
                  {})
def KGPT_input(request) :
    return render(request,
                  "GPTapp/KGPT_2.html",
                  {})

def generate_text(request):
  
    text = request.POST.get('prompt')
    
    tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') 
    tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o")
    model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
    input_ids = tokenizer.encode(text)
    # return HttpResponse(input_ids)   
    gen_ids = model.generate(torch.tensor([input_ids]),
                                max_length=128,
                                repetition_penalty=2.0,
                                pad_token_id=tokenizer.pad_token_id,
                                eos_token_id=tokenizer.eos_token_id,
                                bos_token_id=tokenizer.bos_token_id,
                                use_cache=True)
    generated = tokenizer.decode(gen_ids[0,:].tolist())
    return render(request, 'GPTapp/KGPT_2.html', {'generated_text': generated ,"prompt" : text})

def chatBot(request):
    Q_TKN = "<usr>"   # ì‚¬ìš©ì(ì§ˆë¬¸ì)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìŠ¤í˜ì…œ í† í°ì…ë‹ˆë‹¤.
    A_TKN = "<sys>"   # ì‹œìŠ¤í…œ(ì‘ë‹µì)ì„ ë‚˜íƒ€ë‚´ëŠ” ìŠ¤í˜ì…œ í† í°ì…ë‹ˆë‹¤.
    BOS = '</s>'      # ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” ìŠ¤í˜ì…œ í† í°ì…ë‹ˆë‹¤.
    EOS = '</s>'      # ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” ìŠ¤í˜ì…œ í† í°ì…ë‹ˆë‹¤.
    MASK = '<unused0>'# ë§ˆìŠ¤í‚¹ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ìŠ¤í˜ì…œ í† í°ì…ë‹ˆë‹¤.
    SENT = '<unused1>'# ë‹¤ì¤‘ ë¬¸ì¥ ë¶„ë¥˜ ë“±ì— ì‚¬ìš©ë˜ëŠ” ìŠ¤í˜ì…œ í† í°ì…ë‹ˆë‹¤.
    PAD = '<pad>'     # íŒ¨ë”© ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ìŠ¤í˜ì…œ í† í°ì…ë‹ˆë‹¤.
    koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
            bos_token=BOS, eos_token=EOS, unk_token='<unk>',
            pad_token=PAD, mask_token=MASK) 
    model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')
    # ì³‡ë´‡ ë°ì´í„° ë‹¤ìš´ë¡œë“œ 
    urllib.request.urlretrieve(
        "https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv",
        filename="ChatBotData.csv",
    )
    Chatbot_Data = pd.read_csv("ChatBotData.csv")
    # Test ìš©ìœ¼ë¡œ 300ê°œ ë°ì´í„°ë§Œ ì²˜ë¦¬í•œë‹¤.
    Chatbot_Data = Chatbot_Data[:300]
    Chatbot_Data.head()